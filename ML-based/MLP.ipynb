{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5879c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_curve, auc, mean_squared_error, \\\n",
    "    r2_score, mean_absolute_error,cohen_kappa_score,accuracy_score,f1_score,matthews_corrcoef,precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def standardize(col):\n",
    "    return (col - np.mean(col)) / np.std(col)\n",
    "\n",
    "# the metrics for classification\n",
    "def statistical(y_true, y_pred, y_pro):\n",
    "    c_mat = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = list(c_mat.flatten())\n",
    "    se = tp / (tp + fn)\n",
    "    sp = tn / (tn + fp)\n",
    "    auc_prc = auc(precision_recall_curve(y_true, y_pro, pos_label=1)[1],\n",
    "                  precision_recall_curve(y_true, y_pro, pos_label=1)[0])\n",
    "    acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "#     acc_skl = accuracy_score(y_true, y_pred)\n",
    "    auc_roc = roc_auc_score(y_true, y_pro)\n",
    "    recall = se\n",
    "#     recall_skl = recall_score(y_true, y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "#     precision_skl = precision_score(y_true, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "#     f1_skl = f1_score(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true,y_pred)\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) + 1e-8)\n",
    "#     mcc_skl = matthews_corrcoef(y_true,y_pred)\n",
    "    return tn,fp,fn,tp,se,sp,auc_prc,acc,auc_roc,recall,precision,f1,kappa,mcc\n",
    "\n",
    "def all_one_zeros(data):\n",
    "    if (len(np.unique(data)) == 2):\n",
    "        flag = False\n",
    "    else:\n",
    "        flag = True\n",
    "    return flag\n",
    "\n",
    "\n",
    "feature_selection = False\n",
    "tasks_dic = {'1-AR-Alva-6108-slim-Normalize-group.csv': ['activity']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d2bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'actual_estimator__alpha': LogUniformDistribution(high=0.9999999999, low=1e-10),\n",
    "#  'actual_estimator__hidden_layer_size_0': IntUniformDistribution(high=100, low=50, step=1),\n",
    "#  'actual_estimator__hidden_layer_size_1': IntUniformDistribution(high=100, low=0, step=1),\n",
    "#  'actual_estimator__hidden_layer_size_2': IntUniformDistribution(high=100, low=0, step=1),\n",
    "#  'actual_estimator__learning_rate': CategoricalDistribution(choices=('constant', 'invscaling', 'adaptive')),\n",
    "#  'actual_estimator__activation': CategoricalDistribution(choices=('tanh', 'identity', 'logistic', 'relu'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6daf58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '1-AR-Alva-6108-slim-Normalize-group.csv'\n",
    "task_type = 'cla'  # 'reg' or 'cla'\n",
    "dataset_label = file_name.split('/')[-1].split('_')[0]\n",
    "tasks = tasks_dic[dataset_label]\n",
    "OPT_ITERS = 50\n",
    "repetitions = 10\n",
    "num_pools = 10\n",
    "unbalance = True\n",
    "patience = 100\n",
    "ecfp = True\n",
    "# preset the hyper_parameters_space \n",
    "space_ = {'alpha': hp.loguniform('alpha', 1e-10, 0.9999999999),\n",
    "          'hidden_layer_sizes': hp.choice('hidden_layer_sizes', [(50, 100, 200), (50, 50, 50), (100, 100, 100), (200, 200, 200)]),\n",
    "          'learning_rate': hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "          'activation': hp.choice('activation', ['tanh', 'identity', 'logistic', 'relu'])\n",
    "          }\n",
    "\n",
    "hidden_layer_sizes_ls = [(50, 100, 200), (50, 50, 50), (100, 100, 100), (200, 200, 200)]\n",
    "learning_rate_ls = ['constant', 'invscaling', 'adaptive']\n",
    "activation_ls = ['tanh', 'identity', 'logistic', 'relu']\n",
    "dataset = pd.read_csv(file_name)\n",
    "pd_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5508043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the num of retained features for the 1-AR-Alva-6108-slim-Normalize-group.csv activity is: 1508\n",
      "the best hyper-parameters for 1-AR-Alva-6108-slim-Normalize-group.csv activity are:   {'activation': 3, 'alpha': 1.846530120458842, 'hidden_layer_sizes': 3, 'learning_rate': 2}\n",
      "Iteration 1, loss = 3.31821413\n",
      "Iteration 2, loss = 2.23146430\n",
      "Iteration 3, loss = 1.53362719\n",
      "Iteration 4, loss = 1.10138563\n",
      "Iteration 5, loss = 0.81530282\n",
      "Iteration 6, loss = 0.64347114\n",
      "Iteration 7, loss = 0.52157261\n",
      "Iteration 8, loss = 0.44932665\n",
      "Iteration 9, loss = 0.37850446\n",
      "Iteration 10, loss = 0.34686826\n",
      "Iteration 11, loss = 0.32115413\n",
      "Iteration 12, loss = 0.28359719\n",
      "Iteration 13, loss = 0.24679107\n",
      "Iteration 14, loss = 0.23864498\n",
      "Iteration 15, loss = 0.22985533\n",
      "Iteration 16, loss = 0.21703934\n",
      "Iteration 17, loss = 0.23484037\n",
      "Iteration 18, loss = 0.21679995\n",
      "Iteration 19, loss = 0.21177237\n",
      "Iteration 20, loss = 0.20185514\n",
      "Iteration 21, loss = 0.18852852\n",
      "Iteration 22, loss = 0.17425126\n",
      "Iteration 23, loss = 0.18153500\n",
      "Iteration 24, loss = 0.17702361\n",
      "Iteration 25, loss = 0.17522790\n",
      "Iteration 26, loss = 0.15806393\n",
      "Iteration 27, loss = 0.15241840\n",
      "Iteration 28, loss = 0.15318119\n",
      "Iteration 29, loss = 0.14901438\n",
      "Iteration 30, loss = 0.15692303\n",
      "Iteration 31, loss = 0.15317819\n",
      "Iteration 32, loss = 0.14489431\n",
      "Iteration 33, loss = 0.15210306\n",
      "Iteration 34, loss = 0.14825984\n",
      "Iteration 35, loss = 0.16496259\n",
      "Iteration 36, loss = 0.15184151\n",
      "Iteration 37, loss = 0.14673559\n",
      "Iteration 38, loss = 0.14019365\n",
      "Iteration 39, loss = 0.13762291\n",
      "Iteration 40, loss = 0.13757021\n",
      "Iteration 41, loss = 0.13568476\n",
      "Iteration 42, loss = 0.12998872\n",
      "Iteration 43, loss = 0.13138476\n",
      "Iteration 44, loss = 0.13511302\n",
      "Iteration 45, loss = 0.14288949\n",
      "Iteration 46, loss = 0.13085495\n",
      "Iteration 47, loss = 0.12969765\n",
      "Iteration 48, loss = 0.13007435\n",
      "Iteration 49, loss = 0.12876544\n",
      "Iteration 50, loss = 0.12170886\n",
      "Iteration 51, loss = 0.11973255\n",
      "Iteration 52, loss = 0.11581687\n",
      "Iteration 53, loss = 0.12125533\n",
      "Iteration 54, loss = 0.11533577\n",
      "Iteration 55, loss = 0.11141835\n",
      "Iteration 56, loss = 0.12191150\n",
      "Iteration 57, loss = 0.13051514\n",
      "Iteration 58, loss = 0.14281627\n",
      "Iteration 59, loss = 0.14516705\n",
      "Iteration 60, loss = 0.14274432\n",
      "Iteration 61, loss = 0.13397303\n",
      "Iteration 62, loss = 0.12298700\n",
      "Iteration 63, loss = 0.11962763\n",
      "Iteration 64, loss = 0.12591533\n",
      "Iteration 65, loss = 0.12232455\n",
      "Iteration 66, loss = 0.11476645\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "train 0.9998589873062069 0.9993607818387238\n",
      "valid 0.9515034777956126 0.871599126137301\n",
      "test 0.9343957095033135 0.8200160107151246\n"
     ]
    }
   ],
   "source": [
    "def hyper_runing(subtask):\n",
    "    cols = [subtask]\n",
    "    cols.extend(dataset.columns[(len(tasks) + 1):])\n",
    "    sub_dataset = dataset[cols]\n",
    "\n",
    "    # detect the na in the subtask (y cloumn)\n",
    "    rm_index = sub_dataset[subtask][sub_dataset[subtask].isnull()].index\n",
    "    sub_dataset.drop(index=rm_index, inplace=True)\n",
    "\n",
    "    # remove the features with na\n",
    "    sub_dataset = sub_dataset.dropna(axis=1)\n",
    "    # *******************\n",
    "    # demension reduction\n",
    "    # *******************\n",
    "    # Removing features with low variance\n",
    "    # threshold = 0.05\n",
    "    data_fea_var = sub_dataset.iloc[:, 2:].var()\n",
    "    del_fea1 = list(data_fea_var[data_fea_var <= 0.05].index)\n",
    "    sub_dataset.drop(columns=del_fea1, inplace=True)\n",
    "\n",
    "    # pair correlations\n",
    "    # threshold = 0.95\n",
    "    data_fea_corr = sub_dataset.iloc[:, 2:].corr()\n",
    "    del_fea2_col = []\n",
    "    del_fea2_ind = []\n",
    "    length = data_fea_corr.shape[1]\n",
    "    for i in range(length):\n",
    "        for j in range(i + 1, length):\n",
    "            if abs(data_fea_corr.iloc[i, j]) >= 0.95:\n",
    "                del_fea2_col.append(data_fea_corr.columns[i])\n",
    "                del_fea2_ind.append(data_fea_corr.index[j])\n",
    "    sub_dataset.drop(columns=del_fea2_ind, inplace=True)\n",
    "\n",
    "    # standardize the features\n",
    "    cols_ = list(sub_dataset.columns)[2:]\n",
    "    if not ecfp :\n",
    "        sub_dataset[cols_] = sub_dataset[cols_].apply(standardize, axis=0)\n",
    "\n",
    "    # get the attentivefp data splits\n",
    "    data_tr = sub_dataset[sub_dataset['group'] == 'train']\n",
    "    data_va = sub_dataset[sub_dataset['group'] == 'valid']\n",
    "    data_te = sub_dataset[sub_dataset['group'] == 'test']\n",
    "\n",
    "    # prepare data for training\n",
    "    # training set\n",
    "    data_tr_y = data_tr[subtask].values.reshape(-1, 1)\n",
    "    data_tr_x = np.array(data_tr.iloc[:, 2:].values)\n",
    "\n",
    "    # validation set\n",
    "    data_va_y = data_va[subtask].values.reshape(-1, 1)\n",
    "    data_va_x = np.array(data_va.iloc[:, 2:].values)\n",
    "\n",
    "    # test set\n",
    "    data_te_y = data_te[subtask].values.reshape(-1, 1)\n",
    "    data_te_x = np.array(data_te.iloc[:, 2:].values)\n",
    "\n",
    "    if feature_selection:\n",
    "        # univariate feature selection\n",
    "        trans1 = SelectPercentile(f_classif, percentile=80)\n",
    "        trans1.fit(data_tr_x, data_tr_y)\n",
    "        data_tr_x = trans1.transform(data_tr_x)\n",
    "        data_va_x = trans1.transform(data_va_x)\n",
    "        data_te_x = trans1.transform(data_te_x)\n",
    "\n",
    "        # select from model\n",
    "        clf = XGBClassifier(random_state=1)\n",
    "        clf = clf.fit(data_tr_x, data_tr_y)\n",
    "        trans2 = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "        data_tr_x = trans2.transform(data_tr_x)\n",
    "        data_va_x = trans2.transform(data_va_x)\n",
    "        data_te_x = trans2.transform(data_te_x)\n",
    "\n",
    "    num_fea = data_tr_x.shape[1]\n",
    "    print('the num of retained features for the ' + dataset_label + ' ' + subtask + ' is:', num_fea)\n",
    "\n",
    "    def hyper_opt(args):\n",
    "        model = MLPClassifier(**args, random_state=1) if task_type == 'cla' else MLPRegressor(**args,\n",
    "                                                                                                   random_state=1)\n",
    "\n",
    "        model.fit(data_tr_x, data_tr_y)\n",
    "        val_preds = model.predict_proba(data_va_x) if task_type == 'cla' else \\\n",
    "            model.predict(data_va_x)\n",
    "        loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1]) if task_type == 'cla' else np.sqrt(\n",
    "            mean_squared_error(data_va_y, val_preds))\n",
    "        return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "    # start hyper-parameters optimization\n",
    "    trials = Trials()\n",
    "    best_results = fmin(hyper_opt, space_, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials, show_progressbar=False)\n",
    "    print('the best hyper-parameters for ' + dataset_label + ' ' + subtask + ' are:  ', best_results)\n",
    "      \n",
    "    best_model = MLPClassifier(\n",
    "                                alpha= best_results['alpha'],\n",
    "                                hidden_layer_sizes=hidden_layer_sizes_ls[best_results['hidden_layer_sizes']],\n",
    "                                learning_rate=learning_rate_ls[best_results['learning_rate']],\n",
    "                                activation=activation_ls[best_results['activation']],\n",
    "                                random_state=1, verbose=-1) \\\n",
    "        if task_type == 'cla' else MLPRegressor(\n",
    "                                alpha= best_results['alpha'],\n",
    "                                hidden_layer_sizes=hidden_layer_sizes_ls[best_results['hidden_layer_sizes']],\n",
    "                                learning_rate=learning_rate_ls[best_results['learning_rate']],\n",
    "                                activation=activation_ls[best_results['activation']],\n",
    "                                random_state=1, verbose=-1)  \n",
    "    \n",
    "    best_model.fit(data_tr_x, data_tr_y)\n",
    "    \n",
    "    num_of_compounds = len(sub_dataset)\n",
    "\n",
    "    if task_type == 'cla':\n",
    "        # training set\n",
    "        tr_pred = best_model.predict_proba(data_tr_x)\n",
    "        tr_results = [dataset_label, subtask, 'tr', num_fea, num_of_compounds, data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0] / data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      best_results['alpha'],\n",
    "                      hidden_layer_sizes_ls[best_results['hidden_layer_sizes']],\n",
    "                      learning_rate_ls[best_results['learning_rate']],\n",
    "                      activation_ls[best_results['activation']]\n",
    "                     ]\n",
    "        tr_results.extend(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n",
    "        # validation set\n",
    "        va_pred = best_model.predict_proba(data_va_x)\n",
    "                      \n",
    "        va_results = [dataset_label, subtask, 'va', num_fea, num_of_compounds, data_va_y[data_va_y == 1].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0] / data_va_y[data_va_y == 1].shape[0],\n",
    "                      best_results['alpha'],\n",
    "                      hidden_layer_sizes_ls[best_results['hidden_layer_sizes']],\n",
    "                      learning_rate_ls[best_results['learning_rate']],\n",
    "                      activation_ls[best_results['activation']]\n",
    "                     ]\n",
    "        va_results.extend(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n",
    "\n",
    "        # test set\n",
    "        te_pred = best_model.predict_proba(data_te_x)\n",
    "        te_results = [dataset_label, subtask, 'te', num_fea, num_of_compounds, data_te_y[data_te_y == 1].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0] / data_te_y[data_te_y == 1].shape[0],\n",
    "                      best_results['alpha'],\n",
    "                      hidden_layer_sizes_ls[best_results['hidden_layer_sizes']],\n",
    "                      learning_rate_ls[best_results['learning_rate']],\n",
    "                      activation_ls[best_results['activation']]\n",
    "                     ]\n",
    "        te_results.extend(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n",
    "    else:\n",
    "        # training set\n",
    "        tr_pred = best_model.predict(data_tr_x)\n",
    "        tr_results = [dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      best_results['alpha'],\n",
    "                      hidden_layer_sizes_ls[best_results['hidden_layer_sizes']],\n",
    "                      learning_rate_ls[best_results['learning_rate']],\n",
    "                      activation_ls[best_results['activation']],\n",
    "                      np.sqrt(mean_squared_error(data_tr_y, tr_pred)), r2_score(data_tr_y, tr_pred),\n",
    "                      mean_absolute_error(data_tr_y, tr_pred)]\n",
    "\n",
    "        # validation set\n",
    "        va_pred = best_model.predict(data_va_x)\n",
    "        va_results = [dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      best_results['alpha'],\n",
    "                      hidden_layer_sizes_ls[best_results['hidden_layer_sizes']],\n",
    "                      learning_rate_ls[best_results['learning_rate']],\n",
    "                      activation_ls[best_results['activation']],\n",
    "                      np.sqrt(mean_squared_error(data_va_y, va_pred)), r2_score(data_va_y, va_pred),\n",
    "                      mean_absolute_error(data_va_y, va_pred)]\n",
    "\n",
    "        # test set\n",
    "        te_pred = best_model.predict(data_te_x)\n",
    "        te_results = [dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      best_results['alpha'],\n",
    "                      hidden_layer_sizes_ls[best_results['hidden_layer_sizes']],\n",
    "                      learning_rate_ls[best_results['learning_rate']],\n",
    "                      activation_ls[best_results['activation']],\n",
    "                      np.sqrt(mean_squared_error(data_te_y, te_pred)), r2_score(data_te_y, te_pred),\n",
    "                      mean_absolute_error(data_te_y, te_pred)]\n",
    "    return tr_results, va_results, te_results\n",
    "\n",
    "\n",
    "pool = multiprocessing.Pool(num_pools)\n",
    "res = pool.starmap(hyper_runing, zip(tasks))\n",
    "pool.close()\n",
    "pool.join()\n",
    "for item in res:\n",
    "    for i in range(3):\n",
    "        pd_res.append(item[i])\n",
    "if task_type == 'cla':       \n",
    "                            \n",
    "    best_hyper = pd.DataFrame(pd_res, columns=['dataset', 'subtask', 'set',\n",
    "                                               'num_of_retained_feature',\n",
    "                                               'num_of_compounds', 'postives',\n",
    "                                               'negtives', 'negtives/postives',\n",
    "                                               'alpha', \n",
    "                                               'hidden_layer_sizes',\n",
    "                                               'learning_rate','activation',\n",
    "                                               'tn', 'fp', 'fn', 'tp', 'se', 'sp',\n",
    "                                               'auc_prc', 'acc', 'auc_roc','recall','precision','f1','kappa','mcc'])\n",
    "else:\n",
    "    best_hyper = pd.DataFrame(pd_res, columns=['dataset', 'subtask', 'set',\n",
    "                                               'alpha', \n",
    "                                               'hidden_layer_sizes',\n",
    "                                               'learning_rate','activation',\n",
    "                                               'rmse', 'r2', 'mae'])\n",
    "best_hyper.to_csv('./model/' + dataset_label + '_MLP_hyperopt_info.csv', index=0)\n",
    "\n",
    "if task_type == 'cla':\n",
    "    print('train', best_hyper[best_hyper['set'] == 'tr']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'tr']['auc_prc'].mean())\n",
    "    print('valid', best_hyper[best_hyper['set'] == 'va']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'va']['auc_prc'].mean())\n",
    "    print('test', best_hyper[best_hyper['set'] == 'te']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'te']['auc_prc'].mean())\n",
    "else:\n",
    "    print('train', best_hyper[best_hyper['set'] == 'tr']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'tr']['r2'].mean(), best_hyper[best_hyper['set'] == 'tr']['mae'].mean())\n",
    "    print('valid', best_hyper[best_hyper['set'] == 'va']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'va']['r2'].mean(), best_hyper[best_hyper['set'] == 'va']['mae'].mean())\n",
    "    print('test', best_hyper[best_hyper['set'] == 'te']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'te']['r2'].mean(), best_hyper[best_hyper['set'] == 'te']['mae'].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e5b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 repetitions based on thr best hypers\n",
    "dataset.drop(columns=['group'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49892864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed used in repetition 10 is 10\n",
      "random seed used in repetition 8 is 8\n",
      "random seed used in repetition 3 is 3\n",
      "random seed used in repetition 9 is 9\n",
      "random seed used in repetition 7 is 7\n",
      "random seed used in repetition 1 is 1\n",
      "random seed used in repetition 5 is 5\n",
      "random seed used in repetition 4 is 4\n",
      "random seed used in repetition 2 is 2\n",
      "random seed used in repetition 6 is 6\n",
      "1-AR-Alva-6108-slim-Normalize-group.csv_MLP: the mean auc_roc for the training set is 0.999 with std 0.001\n",
      "1-AR-Alva-6108-slim-Normalize-group.csv_MLP: the mean auc_roc for the validation set is 0.925 with std 0.019\n",
      "1-AR-Alva-6108-slim-Normalize-group.csv_MLP: the mean auc_roc for the test set is 0.914 with std 0.017\n"
     ]
    }
   ],
   "source": [
    "pd_res = []\n",
    "def best_model_runing(split):\n",
    "    seed = split\n",
    "    if task_type == 'cla':\n",
    "        while True:\n",
    "            training_data, data_te = train_test_split(sub_dataset, test_size=0.1, random_state=seed)\n",
    "            # the training set was further splited into the training set and validation set\n",
    "            data_tr, data_va = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "            if (all_one_zeros(data_tr[subtask]) or all_one_zeros(data_va[subtask]) or all_one_zeros(data_te[subtask])):\n",
    "                print(\n",
    "                    '\\ninvalid random seed {} due to one class presented in the {} splitted sets...'.format(seed,\n",
    "                                                                                                            subtask))\n",
    "                print('Changing to another random seed...\\n')\n",
    "                seed = np.random.randint(10, 999999)\n",
    "            else:\n",
    "                print('random seed used in repetition {} is {}'.format(split, seed))\n",
    "                break\n",
    "    else:\n",
    "        training_data, data_te = train_test_split(sub_dataset, test_size=0.1, random_state=seed)\n",
    "        # the training set was further splited into the training set and validation set\n",
    "        data_tr, data_va = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "\n",
    "    # prepare data for training\n",
    "    # training set\n",
    "    data_tr_y = data_tr[subtask].values.reshape(-1, 1)\n",
    "    data_tr_x = np.array(data_tr.iloc[:, 1:].values)\n",
    "\n",
    "    # validation set\n",
    "    data_va_y = data_va[subtask].values.reshape(-1, 1)\n",
    "    data_va_x = np.array(data_va.iloc[:, 1:].values)\n",
    "\n",
    "    # test set\n",
    "    data_te_y = data_te[subtask].values.reshape(-1, 1)\n",
    "    data_te_x = np.array(data_te.iloc[:, 1:].values)\n",
    "\n",
    "    if feature_selection:\n",
    "        # univariate feature selection\n",
    "        trans1 = SelectPercentile(f_classif, percentile=80)\n",
    "        trans1.fit(data_tr_x, data_tr_y)\n",
    "        data_tr_x = trans1.transform(data_tr_x)\n",
    "        data_va_x = trans1.transform(data_va_x)\n",
    "        data_te_x = trans1.transform(data_te_x)\n",
    "\n",
    "        # select from model\n",
    "        clf = XGBClassifier(random_state=1)\n",
    "        clf = clf.fit(data_tr_x, data_tr_y)\n",
    "        trans2 = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "        data_tr_x = trans2.transform(data_tr_x)\n",
    "        data_va_x = trans2.transform(data_va_x)\n",
    "        data_te_x = trans2.transform(data_te_x)\n",
    "\n",
    "    num_fea = data_tr_x.shape[1]\n",
    "    pos_weight = (len(sub_dataset) - sum(sub_dataset[subtask])) / sum(sub_dataset[subtask])\n",
    "    model = MLPClassifier(\n",
    "                          alpha=best_hyper[best_hyper.subtask == subtask].iloc[0,]['alpha'],\n",
    "                          hidden_layer_sizes=best_hyper[best_hyper.subtask == subtask].iloc[0,]['hidden_layer_sizes'],\n",
    "                          learning_rate=best_hyper[best_hyper.subtask == subtask].iloc[0,]['learning_rate'],\n",
    "                          activation=best_hyper[best_hyper.subtask == subtask].iloc[0,]['activation'],\n",
    "                          random_state=1) \\\n",
    "        if task_type == 'cla' else MLPRegressor(\n",
    "                          alpha=best_hyper[best_hyper.subtask == subtask].iloc[0,]['alpha'],\n",
    "                          hidden_layer_sizes=best_hyper[best_hyper.subtask == subtask].iloc[0,]['hidden_layer_sizes'],\n",
    "                          learning_rate=best_hyper[best_hyper.subtask == subtask].iloc[0,]['learning_rate'],\n",
    "                          activation=best_hyper[best_hyper.subtask == subtask].iloc[0,]['activation'],\n",
    "        random_state=1, seed=1)\n",
    "\n",
    "    model.fit(data_tr_x, data_tr_y)\n",
    "    num_of_compounds = sub_dataset.shape[0]\n",
    "    import pickle\n",
    "    pickle.dump(model, open(\"./model/mlp_\"+str(split)+\".pkl\", \"wb\"))\n",
    "    if task_type == 'cla':\n",
    "        # training set\n",
    "        tr_pred = model.predict_proba(data_tr_x)\n",
    "        tr_results = [split, dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0] / data_tr_y[data_tr_y == 1].shape[0]]\n",
    "        tr_results.extend(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n",
    "\n",
    "        # validation set\n",
    "        va_pred = model.predict_proba(data_va_x)\n",
    "        va_results = [split, dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      data_va_y[data_va_y == 1].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0] / data_va_y[data_va_y == 1].shape[0]]\n",
    "        va_results.extend(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n",
    "\n",
    "        # test set\n",
    "        te_pred = model.predict_proba(data_te_x)\n",
    "        te_results = [split, dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      data_te_y[data_te_y == 1].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0] / data_te_y[data_te_y == 1].shape[0]]\n",
    "        te_results.extend(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n",
    "    else:\n",
    "        # training set\n",
    "        tr_pred = model.predict(data_tr_x)\n",
    "        tr_results = [split, dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_tr_y, tr_pred)), r2_score(data_tr_y, tr_pred),\n",
    "                      mean_absolute_error(data_tr_y, tr_pred)]\n",
    "\n",
    "        # validation set\n",
    "        va_pred = model.predict(data_va_x)\n",
    "        va_results = [split, dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_va_y, va_pred)), r2_score(data_va_y, va_pred),\n",
    "                      mean_absolute_error(data_va_y, va_pred)]\n",
    "\n",
    "        # test set\n",
    "        te_pred = model.predict(data_te_x)\n",
    "        te_results = [split, dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_te_y, te_pred)), r2_score(data_te_y, te_pred),\n",
    "                      mean_absolute_error(data_te_y, te_pred)]\n",
    "    return tr_results, va_results, te_results\n",
    "\n",
    "\n",
    "for subtask in tasks:\n",
    "    cols = [subtask]\n",
    "    cols.extend(dataset.columns[(len(tasks) + 1):])\n",
    "    sub_dataset = dataset[cols]\n",
    "\n",
    "    # detect the NA in the subtask (y cloumn)\n",
    "    rm_index = sub_dataset[subtask][sub_dataset[subtask].isnull()].index\n",
    "    sub_dataset.drop(index=rm_index, inplace=True)\n",
    "\n",
    "    # remove the features with na\n",
    "    sub_dataset = sub_dataset.dropna(axis=1)\n",
    "    # *******************\n",
    "    # demension reduction\n",
    "    # *******************\n",
    "    # Removing features with low variance\n",
    "    # threshold = 0.05\n",
    "    data_fea_var = sub_dataset.iloc[:, 1:].var()\n",
    "    del_fea1 = list(data_fea_var[data_fea_var <= 0.05].index)\n",
    "    sub_dataset.drop(columns=del_fea1, inplace=True)\n",
    "\n",
    "    # pair correlations\n",
    "    # threshold = 0.95\n",
    "    data_fea_corr = sub_dataset.iloc[:, 1:].corr()\n",
    "    del_fea2_col = []\n",
    "    del_fea2_ind = []\n",
    "    length = data_fea_corr.shape[1]\n",
    "    for i in range(length):\n",
    "        for j in range(i + 1, length):\n",
    "            if abs(data_fea_corr.iloc[i, j]) >= 0.95:\n",
    "                del_fea2_col.append(data_fea_corr.columns[i])\n",
    "                del_fea2_ind.append(data_fea_corr.index[j])\n",
    "    sub_dataset.drop(columns=del_fea2_ind, inplace=True)\n",
    "\n",
    "    # standardize the features\n",
    "    cols_ = list(sub_dataset.columns)[1:]\n",
    "    \n",
    "    if not ecfp :\n",
    "        sub_dataset[cols_] = sub_dataset[cols_].apply(standardize, axis=0)\n",
    "\n",
    "    # for split in range(1, splits+1):\n",
    "    pool = multiprocessing.Pool(num_pools)\n",
    "    res = pool.starmap(best_model_runing, zip(range(1, repetitions + 1)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for item in res:\n",
    "        for i in range(3):\n",
    "            pd_res.append(item[i])\n",
    "if task_type == 'cla':\n",
    "    stat_res = pd.DataFrame(pd_res, columns=['split', 'dataset', 'subtask', 'set',\n",
    "                                             'num_of_retained_feature',\n",
    "                                             'num_of_compounds', 'postives',\n",
    "                                             'negtives', 'negtives/postives',\n",
    "                                             'tn', 'fp', 'fn', 'tp', 'se', 'sp',\n",
    "                                             'auc_prc', 'acc', 'auc_roc','recall','precision','f1','kappa','mcc'])\n",
    "else:\n",
    "    stat_res = pd.DataFrame(pd_res, columns=['split', 'dataset', 'subtask', 'set',\n",
    "                                             'num_of_retained_feature',\n",
    "                                             'num_of_compounds', 'rmse', 'r2', 'mae'])\n",
    "stat_res.to_csv('./model/' + dataset_label + '_MLP_statistical_results_split.csv', index=0)\n",
    "# single tasks\n",
    "if len(tasks) == 1:\n",
    "    args = {'data_label': dataset_label, 'metric': 'auc_roc' if task_type == 'cla' else 'rmse', 'model': 'MLP'}\n",
    "    print('{}_{}: the mean {} for the training set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(\n",
    "            stat_res[stat_res['set'] == 'tr'][args['metric']]), np.std(\n",
    "            stat_res[stat_res['set'] == 'tr'][args['metric']])))\n",
    "    print(\n",
    "        '{}_{}: the mean {} for the validation set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(\n",
    "                stat_res[stat_res['set'] == 'va'][args['metric']]), np.std(\n",
    "                stat_res[stat_res['set'] == 'va'][args['metric']])))\n",
    "    print('{}_{}: the mean {} for the test set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                 args['metric'], np.mean(\n",
    "            stat_res[stat_res['set'] == 'te'][args['metric']]), np.std(\n",
    "            stat_res[stat_res['set'] == 'te'][args['metric']])))\n",
    "# multi-tasks\n",
    "else:\n",
    "    args = {'data_label': dataset_label, 'metric': 'auc_roc' if dataset_label != 'muv' else 'auc_prc', 'model': 'MLP'}\n",
    "    tr_acc = np.zeros(repetitions)\n",
    "    va_acc = np.zeros(repetitions)\n",
    "    te_acc = np.zeros(repetitions)\n",
    "    for subtask in tasks:\n",
    "        tr = stat_res[stat_res['set'] == 'tr']\n",
    "        tr_acc = tr_acc + tr[tr['subtask'] == subtask][args['metric']].values\n",
    "\n",
    "        va = stat_res[stat_res['set'] == 'va']\n",
    "        va_acc = va_acc + va[va['subtask'] == subtask][args['metric']].values\n",
    "\n",
    "        te = stat_res[stat_res['set'] == 'te']\n",
    "        te_acc = te_acc + te[te['subtask'] == subtask][args['metric']].values\n",
    "    tr_acc = tr_acc / len(tasks)\n",
    "    va_acc = va_acc / len(tasks)\n",
    "    te_acc = te_acc / len(tasks)\n",
    "    print('{}_{}: the mean {} for the training set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(tr_acc),\n",
    "                                                                                     np.std(tr_acc)))\n",
    "    print(\n",
    "        '{}_{}: the mean {} for the validation set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(va_acc),\n",
    "                                                                                     np.std(va_acc)))\n",
    "    print('{}_{}: the mean {} for the test set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                 args['metric'], np.mean(te_acc),\n",
    "                                                                                 np.std(te_acc)))\n",
    "end = time.time()  # get the end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50bc4eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the elapsed time is: 1.5031298385063807 H\n"
     ]
    }
   ],
   "source": [
    "# acc auc_roc recall precision f1 kappa mcc\n",
    "acc_str = 'acc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['acc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['acc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['acc']),\n",
    ")\n",
    "auc_str = 'auc_roc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['auc_roc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['auc_roc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['auc_roc']),\n",
    ")\n",
    "recall_str = 'recall of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['recall']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['recall']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['recall']),\n",
    ")\n",
    "precision_str = 'precision of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['precision']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['precision']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['precision']),\n",
    ")\n",
    "f1_str = 'f1 of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['f1']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['f1']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['f1']),\n",
    ")\n",
    "kappa_str = 'kappa of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['kappa']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['kappa']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['kappa']),\n",
    ")\n",
    "mcc_str = 'mcc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['mcc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['mcc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['mcc']),\n",
    ")\n",
    "print('the elapsed time is:', (end - start)/3600, 'H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfb4f708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc of training set is 0.993±0.004, validation set is 0.911±0.011, test set is 0.903±0.008\n",
      "auc_roc of training set is 0.999±0.001, validation set is 0.925±0.019, test set is 0.914±0.017\n",
      "recall of training set is 0.985±0.011, validation set is 0.748±0.056, test set is 0.730±0.035\n",
      "precision of training set is 0.981±0.013, validation set is 0.785±0.038, test set is 0.763±0.048\n",
      "f1 of training set is 0.983±0.009, validation set is 0.764±0.027, test set is 0.745±0.022\n",
      "kappa of training set is 0.979±0.012, validation set is 0.709±0.032, test set is 0.685±0.026\n",
      "mcc of training set is 0.979±0.011, validation set is 0.710±0.032, test set is 0.686±0.026\n"
     ]
    }
   ],
   "source": [
    "print(acc_str)\n",
    "print(auc_str)\n",
    "print(recall_str)\n",
    "print(precision_str)\n",
    "print(f1_str)\n",
    "print(kappa_str)\n",
    "print(mcc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee4ebafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/output_mlp.txt', 'w') as f:\n",
    "    f.write(acc_str+'\\n')\n",
    "    f.write(auc_str+'\\n')\n",
    "    f.write(recall_str+'\\n')\n",
    "    f.write(precision_str+'\\n')\n",
    "    f.write(f1_str+'\\n')\n",
    "    f.write(kappa_str+'\\n')\n",
    "    f.write(mcc_str+'\\n')\n",
    "    f.write(str(cols_)+'\\n')\n",
    "cols_ = pd.DataFrame(cols_)\n",
    "cols_.to_csv('output/output_mlp_cols.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaad1fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model: MLP</th>\n",
       "      <th>Train</th>\n",
       "      <th>Tr_STD</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Va_STD</th>\n",
       "      <th>Test</th>\n",
       "      <th>Te_STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acc</td>\n",
       "      <td>0.993471</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.910727</td>\n",
       "      <td>0.011251</td>\n",
       "      <td>0.902782</td>\n",
       "      <td>0.008255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auc_roc</td>\n",
       "      <td>0.999383</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.924671</td>\n",
       "      <td>0.018575</td>\n",
       "      <td>0.913816</td>\n",
       "      <td>0.017420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.984667</td>\n",
       "      <td>0.011162</td>\n",
       "      <td>0.747685</td>\n",
       "      <td>0.056315</td>\n",
       "      <td>0.729798</td>\n",
       "      <td>0.034847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.981251</td>\n",
       "      <td>0.013211</td>\n",
       "      <td>0.784621</td>\n",
       "      <td>0.037709</td>\n",
       "      <td>0.763415</td>\n",
       "      <td>0.048326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.982889</td>\n",
       "      <td>0.009295</td>\n",
       "      <td>0.763707</td>\n",
       "      <td>0.027023</td>\n",
       "      <td>0.744502</td>\n",
       "      <td>0.022312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kappa</td>\n",
       "      <td>0.978855</td>\n",
       "      <td>0.011515</td>\n",
       "      <td>0.708726</td>\n",
       "      <td>0.031586</td>\n",
       "      <td>0.684606</td>\n",
       "      <td>0.025852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mcc</td>\n",
       "      <td>0.978908</td>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.710462</td>\n",
       "      <td>0.031889</td>\n",
       "      <td>0.686063</td>\n",
       "      <td>0.025684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>auc_prc</td>\n",
       "      <td>0.997197</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.843261</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>0.809937</td>\n",
       "      <td>0.028615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model: MLP      Train    Tr_STD  Validation    Va_STD      Test    Te_STD\n",
       "0         acc  0.993471  0.003585    0.910727  0.011251  0.902782  0.008255\n",
       "1     auc_roc  0.999383  0.000605    0.924671  0.018575  0.913816  0.017420\n",
       "2      recall  0.984667  0.011162    0.747685  0.056315  0.729798  0.034847\n",
       "3   precision  0.981251  0.013211    0.784621  0.037709  0.763415  0.048326\n",
       "4          f1  0.982889  0.009295    0.763707  0.027023  0.744502  0.022312\n",
       "5       kappa  0.978855  0.011515    0.708726  0.031586  0.684606  0.025852\n",
       "6         mcc  0.978908  0.011446    0.710462  0.031889  0.686063  0.025684\n",
       "7     auc_prc  0.997197  0.003395    0.843261  0.026528  0.809937  0.028615"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "dict1 = {\"Model: MLP \":['acc','auc_roc','recall','precision','f1','kappa','mcc','auc_prc'],\n",
    "         \"Train\":[np.mean(stat_res[stat_res['set'] == 'tr']['acc']),np.mean(stat_res[stat_res['set'] == 'tr']['auc_roc']),\n",
    "                  np.mean(stat_res[stat_res['set'] == 'tr']['recall']),np.mean(stat_res[stat_res['set'] == 'tr']['precision']),\n",
    "                  np.mean(stat_res[stat_res['set'] == 'tr']['f1']),np.mean(stat_res[stat_res['set'] == 'tr']['kappa']),\n",
    "                  np.mean(stat_res[stat_res['set'] == 'tr']['mcc']),np.mean(stat_res[stat_res['set'] == 'tr']['auc_prc']),                                     \n",
    "                 ],\n",
    "         \"Tr_STD\":[np.std(stat_res[stat_res['set'] == 'tr']['acc']),np.std(stat_res[stat_res['set'] == 'tr']['auc_roc']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'tr']['recall']),np.std(stat_res[stat_res['set'] == 'tr']['precision']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'tr']['f1']),np.std(stat_res[stat_res['set'] == 'tr']['kappa']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'tr']['mcc']),np.std(stat_res[stat_res['set'] == 'tr']['auc_prc']),],\n",
    "         \"Validation\":[np.mean(stat_res[stat_res['set'] == 'va']['acc']),np.mean(stat_res[stat_res['set'] == 'va']['auc_roc']),\n",
    "                      np.mean(stat_res[stat_res['set'] == 'va']['recall']),np.mean(stat_res[stat_res['set'] == 'va']['precision']),\n",
    "                      np.mean(stat_res[stat_res['set'] == 'va']['f1']),np.mean(stat_res[stat_res['set'] == 'va']['kappa']),\n",
    "                      np.mean(stat_res[stat_res['set'] == 'va']['mcc']),np.mean(stat_res[stat_res['set'] == 'va']['auc_prc'])],\n",
    "         \"Va_STD\":[np.std(stat_res[stat_res['set'] == 'va']['acc']),np.std(stat_res[stat_res['set'] == 'va']['auc_roc']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'va']['recall']),np.std(stat_res[stat_res['set'] == 'va']['precision']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'va']['f1']),np.std(stat_res[stat_res['set'] == 'va']['kappa']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'va']['mcc']),np.std(stat_res[stat_res['set'] == 'va']['auc_prc'])],\n",
    "         \"Test\":[np.mean(stat_res[stat_res['set'] == 'te']['acc']),np.mean(stat_res[stat_res['set'] == 'te']['auc_roc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['recall']),np.mean(stat_res[stat_res['set'] == 'te']['precision']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['f1']),np.mean(stat_res[stat_res['set'] == 'te']['kappa']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['mcc']),np.mean(stat_res[stat_res['set'] == 'te']['auc_prc'])],\n",
    "          \"Te_STD\":[np.std(stat_res[stat_res['set'] == 'te']['acc']),np.std(stat_res[stat_res['set'] == 'te']['auc_roc']),\n",
    "                   np.std(stat_res[stat_res['set'] == 'te']['recall']),np.std(stat_res[stat_res['set'] == 'te']['precision']),\n",
    "                   np.std(stat_res[stat_res['set'] == 'te']['f1']),np.std(stat_res[stat_res['set'] == 'te']['kappa']),\n",
    "                   np.std(stat_res[stat_res['set'] == 'te']['mcc']),np.std(stat_res[stat_res['set'] == 'te']['auc_prc']),]}\n",
    "dict1 = collections.OrderedDict(dict1)\n",
    "df = pd.DataFrame(dict1,index = None)\n",
    "df.to_csv('output/output_mlp.csv',index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b745f58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>1.84653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <td>(200, 200, 200)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>adaptive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activation</th>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Values\n",
       "alpha                       1.84653\n",
       "hidden_layer_sizes  (200, 200, 200)\n",
       "learning_rate              adaptive\n",
       "activation                     relu"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option ( 'display.width', None)\n",
    "pd.set_option ( 'display.max_columns', None) #显示全部列\n",
    "hyper_parameters = best_hyper.iloc[0:1,8:-14].T\n",
    "hyper_parameters.rename(columns={0:'Values'},inplace=True) \n",
    "hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86790fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-dgl]",
   "language": "python",
   "name": "conda-env-anaconda3-dgl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
